{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "import os\n",
    "import re\n",
    "import _pickle as cPickle\n",
    "from collections import OrderedDict, defaultdict, Counter\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import math\n",
    "import pdb\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "\n",
    "from data_structure import Instance\n",
    "from utils import apply_parallel, get_tokens, get_group_df, filter_words\n",
    "\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "pd.set_option('display.max_rows', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-seed', type=int, default=1234)\n",
    "parser.add_argument('-n_reviews', type=int, default=8)\n",
    "parser.add_argument('-item_min_reviews', type=int, default=8)\n",
    "parser.add_argument('-n_per_item', type=int, default=2)\n",
    "parser.add_argument('-filter_doc_l', type=int, default=50)\n",
    "parser.add_argument('-filter_sent_l', type=int, default=40)\n",
    "\n",
    "parser.add_argument('-train_dir', type=str, default='data/amazon/train')\n",
    "parser.add_argument('-dev_path', type=str, default='data/amazon/dev.csv')\n",
    "parser.add_argument('-test_path', type=str, default='data/amazon/test.csv')\n",
    "\n",
    "config = parser.parse_args('')\n",
    "config.output_path = os.path.join('data', 'amazon/amazon_recursum_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens\n",
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = {'cloth': 'clothing', 'electronics': 'electronics', 'health_personal_care': 'health', 'home_kitchen': 'home',\n",
    "                               'reviews_clothing_shoes_and_jewelry': 'clothing', 'reviews_health_and_personal_care': 'health', 'reviews_electronics': 'electronics', 'reviews_home_and_kitchen': 'home'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load val & test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_df(ref_path):\n",
    "    ref_raw_df = pd.read_csv(ref_path, sep='\\t')\n",
    "\n",
    "    ref_list = []\n",
    "    for _, row in ref_raw_df.iterrows():\n",
    "        business_id = row['prod_id']\n",
    "        category = category_dict[row['cat']]\n",
    "        text_list = [row['rev%i'%(i_rev+1)] for i_rev in range(8)]\n",
    "        tokens = [sent_tokens for text in text_list for sent_tokens in get_tokens(text)]\n",
    "        text = ' </DOC> '.join(text_list)\n",
    "        stars = [0 for _ in range(8)]\n",
    "        doc_l = len(tokens)\n",
    "        sent_l = [len(line) for line in tokens]\n",
    "        max_sent_l = max(sent_l)\n",
    "\n",
    "        for i_summ in range(3):\n",
    "            summary = row['summ%i'%(i_summ+1)]\n",
    "            summary_tokens = get_tokens(summary)\n",
    "            summary_doc_l = len(summary_tokens)\n",
    "            summary_max_sent_l = max([len(line) for line in tokens])\n",
    "\n",
    "            ref_list.append({'business_id': business_id, 'category': category, 'text': text, 'tokens': tokens, 'summary': summary, 'stars': stars, \\\n",
    "                                         'doc_l': doc_l, 'max_sent_l':max_sent_l, 'sent_l':sent_l,\n",
    "                                         'summary_tokens': summary_tokens, 'summary_doc_l': summary_doc_l, 'summary_max_sent_l': summary_max_sent_l})\n",
    "\n",
    "    ref_df = pd.DataFrame(ref_list)\n",
    "    return ref_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = get_ref_df(config.dev_path)\n",
    "test_df = get_ref_df(config.test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load train raw df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_raw_df(data_paths):\n",
    "    data_raw_dfs = []\n",
    "    for data_path in data_paths:\n",
    "        item_raw_df = pd.read_csv(data_path, '\\t')\n",
    "        item_raw_df['tokens'] = item_raw_df['review_text'].apply(get_tokens)\n",
    "        item_raw_df = item_raw_df[item_raw_df['tokens'].apply(lambda tokens: len(tokens) > 2)]\n",
    "        if 'None' in list(item_raw_df.rating.values):\n",
    "            item_raw_df = item_raw_df[item_raw_df['rating'].apply(lambda rating: rating != 'None')]\n",
    "            item_raw_df['rating'] = item_raw_df['rating'].apply(lambda r: float(r))\n",
    "        if len(item_raw_df) == 0: continue\n",
    "        data_raw_df = item_raw_df.groupby('group_id').agg({\n",
    "            'category': lambda category_list: category_dict[list(category_list)[0]],\n",
    "            'tokens': lambda tokens_list: list(tokens_list),\n",
    "            'rating': lambda stars_list: list(stars_list)\n",
    "        })\n",
    "        data_raw_dfs.append(data_raw_df)\n",
    "    data_raw_df = pd.concat(data_raw_dfs)\n",
    "    return data_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/amazon/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/amazon/train'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_paths = [os.path.join(config.train_dir, data_name) for data_name in os.listdir(config.train_dir)]\n",
    "train_raw_df = apply_parallel(train_data_paths, num_split=32, map_func=get_data_raw_df).reset_index().rename(columns={'group_id': 'business_id', 'rating': 'stars'})\n",
    "len(train_raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load train df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each item will appear 2 times\n",
      "filtered unbalanced 0 instances from group_df\n",
      "280692\n",
      "CPU times: user 1min 41s, sys: 2.01 s, total: 1min 43s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df = get_group_df(train_tmp_df, n_reviews=config.n_reviews, filter_sent_l=config.filter_sent_l, filter_doc_l=config.filter_doc_l, \\\n",
    "                        item_min_reviews=config.item_min_reviews, n_per_item=config.n_per_item)\n",
    "print(len(train_df)) # 280692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(182928, 140346)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_raw_df.business_id)), len(set(train_df.business_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build token idxs for language modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 s, sys: 4.39 s, total: 27.4 s\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words_list = train_df['tokens'].apply(lambda tokens: [token for line in tokens for token in line])\n",
    "word_tf_dict = sorted(Counter([word for words in words_list for word in words]).items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30732"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_words = [PAD, UNK, BOS, EOS]\n",
    "lm_words += filter_words(word_tf_dict, min_tf=16) # large, usual\n",
    "idx_to_word = {idx: word for idx, word in enumerate(lm_words)}\n",
    "word_to_idx = {word: idx for idx, word in idx_to_word.items()}\n",
    "len(lm_words) # 30732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_token_idxs(tokens_series):\n",
    "    def get_token_idxs(tokens):\n",
    "        return [[word_to_idx[token] if token in word_to_idx else word_to_idx[UNK] for token in sent] for sent in tokens]\n",
    "    return tokens_series.apply(get_token_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.9 s, sys: 28.1 s, total: 1min 13s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df['token_idxs'] = apply_parallel(train_df['tokens'], num_split=64, map_func=apply_token_idxs)\n",
    "dev_df['token_idxs'] = apply_token_idxs(dev_df['tokens'])\n",
    "test_df['token_idxs'] = apply_token_idxs(test_df['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_df(data_df, summary=False):\n",
    "    save_df = data_df[['business_id', 'doc_l', 'sent_l', 'max_sent_l', 'bows', 'tfidfbows', 'token_idxs']]\n",
    "    if summary: save_df = data_df[['business_id', 'doc_l', 'sent_l', 'max_sent_l', 'bows', 'tfidfbows', 'token_idxs', \\\n",
    "                                                               'text', 'summary', 'summary_tokens', 'summary_doc_l', 'summary_max_sent_l']]\n",
    "    return save_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280692, 84, 96)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (280692, 84, 96)\n",
    "train_save_df = get_save_df(train_df)\n",
    "dev_save_df = get_save_df(dev_df, summary=True)\n",
    "test_save_df = get_save_df(test_df, summary=True)\n",
    "len(train_save_df), len(dev_save_df), len(test_save_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/amazon/amazon_df.pkl', 30732)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.output_path, len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving preprocessed instances...\n"
     ]
    }
   ],
   "source": [
    "print('saving preprocessed instances...')\n",
    "cPickle.dump((train_save_df, dev_save_df, test_save_df, word_to_idx, idx_to_word, bow_idxs),open(config.output_path,'wb'))\n",
    "# cPickle.dump((train_save_df, dev_save_df, test_save_df, word_to_idx, idx_to_word, bow_idxs),open(config.output_large_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
