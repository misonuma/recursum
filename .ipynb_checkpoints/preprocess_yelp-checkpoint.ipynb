{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import _pickle as cPickle\n",
    "from collections import OrderedDict, defaultdict, Counter\n",
    "import argparse\n",
    "import multiprocessing\n",
    "import math\n",
    "import pdb\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from data_structure import Instance\n",
    "\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)\n",
    "\n",
    "pd.set_option('display.max_rows', 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-seed', type=int, default=1234)\n",
    "parser.add_argument('-n_reviews', type=int, default=8)\n",
    "parser.add_argument('-item_min_reviews', type=int, default=8)\n",
    "parser.add_argument('-n_per_item', type=int, default=12)\n",
    "parser.add_argument('-filter_doc_l', type=int, default=50)\n",
    "parser.add_argument('-filter_sent_l', type=int, default=40)\n",
    "parser.add_argument('-min_tf', type=int, default=16)\n",
    "\n",
    "parser.add_argument('-train_dir', type=str, default='data/yelp/train')\n",
    "parser.add_argument('-val_dir', type=str, default='data/yelp/val')\n",
    "parser.add_argument('-test_dir', type=str, default='data/yelp/test')\n",
    "parser.add_argument('-ref_path', type=str, default='data/yelp/references.csv')\n",
    "\n",
    "parser.add_argument('-stopwords_path', type=str, default='data/stopwords_mallet.txt')\n",
    "\n",
    "config = parser.parse_args('')\n",
    "config.raw_path = os.path.join('data', 'yelp/yelp_raw_df.pkl')\n",
    "config.output_path = os.path.join('data', 'yelp/yelp_recursum_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# special tokens\n",
    "PAD = '<pad>' # This has a vocab id, which is used to pad the encoder input, decoder input and target sequence\n",
    "UNK = '<unk>' # This has a vocab id, which is used to represent out-of-vocabulary words\n",
    "BOS = '<p>' # This has a vocab id, which is used at the beginning of every decoder input sequence\n",
    "EOS = '</p>' # This has a vocab id, which is used at the end of untruncated target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load train raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_raw_df(data_paths):\n",
    "    data_raw_dfs = []\n",
    "    for data_path in data_paths:\n",
    "        f = open(data_path, 'r')\n",
    "        item_raw_df = pd.read_json(f)\n",
    "        f.close()\n",
    "        data_raw_dfs.append(item_raw_df)\n",
    "    data_raw_df = pd.concat(data_raw_dfs)\n",
    "    data_raw_df['tokens'] = data_raw_df['text'].apply(get_tokens)\n",
    "    data_raw_df = data_raw_df[data_raw_df['tokens'].apply(lambda tokens: len(tokens) > 2)]\n",
    "    return data_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_paths = lambda data_dir: [os.path.join(data_dir, data_name) for data_name in os.listdir(data_dir) if not data_name == 'store-to-nreviews.json']\n",
    "train_data_paths = get_data_paths(config.train_dir)\n",
    "val_data_paths = get_data_paths(config.val_dir)\n",
    "test_data_paths = get_data_paths(config.test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.3 s, sys: 12.6 s, total: 1min 4s\n",
      "Wall time: 7min 13s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2008099"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_raw_df = apply_parallel(train_data_paths, num_split=32, map_func=get_data_raw_df).reset_index()\n",
    "len(train_raw_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load ref raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.19 s, sys: 5.21 s, total: 13.4 s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "val_tmp_df = apply_parallel(val_data_paths, num_split=8, map_func=get_data_raw_df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.01 s, sys: 3.18 s, total: 6.19 s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_tmp_df = apply_parallel(test_data_paths, num_split=8, map_func=get_data_raw_df).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_raw_df(ref_path, train_raw_df, val_tmp_df, test_tmp_df):\n",
    "    def get_review_business_id_dict(ref_df, train_raw_df, val_tmp_df, test_tmp_df):\n",
    "        ref_review_ids = []\n",
    "        for _, row in ref_df.iterrows():\n",
    "            ref_review_ids += [row['Input.original_review_%i_id' % i] for i in range(config.n_reviews)]\n",
    "        ref_review_id_df = pd.DataFrame(ref_review_ids, columns=['review_id']) # only review_id in reference.csv\n",
    "        concat_raw_df = pd.concat([train_raw_df, val_tmp_df, test_tmp_df])[['review_id', 'business_id', 'stars']] # filter review_id and business_id pair\n",
    "        review_business_id_dict = {row.review_id: row.business_id for _, row in pd.merge(ref_review_id_df, concat_raw_df).iterrows()}\n",
    "        review_stars_dict = {row.review_id: row.stars for _, row in pd.merge(ref_review_id_df, concat_raw_df).iterrows()}\n",
    "        return review_business_id_dict, review_stars_dict\n",
    "    \n",
    "    ref_df = pd.read_csv(ref_path)\n",
    "    ref_df['business_id_csv'] = ref_df.apply(lambda row: row['Input.business_id'] if row['Input.business_id'] != '#NAME?' else 'null_%i' % row.name, axis=1)\n",
    "    review_business_id_dict, review_stars_dict = get_review_business_id_dict(ref_df, train_raw_df, val_tmp_df, test_tmp_df)\n",
    "    \n",
    "    ref_raw_dfs = []\n",
    "    for index, row in ref_df.iterrows():\n",
    "        business_id = None \n",
    "        texts, review_ids, stars = [], [], []\n",
    "        summary = row['Answer.summary']\n",
    "        for i in range(config.n_reviews):\n",
    "            text = row['Input.original_review_%i' % i]\n",
    "            review_id = row['Input.original_review_%i_id' % i]\n",
    "            star = review_stars_dict[review_id] if review_id in review_stars_dict else 3\n",
    "            \n",
    "            texts.append(text)\n",
    "            review_ids.append(review_id)\n",
    "            stars.append(star)\n",
    "            \n",
    "            if review_id == '#NAME?': review_id = None\n",
    "            if business_id is None and review_id in review_business_id_dict: business_id = review_business_id_dict[review_id] # get business_id from review_business_id_dict\n",
    "            \n",
    "        if business_id is None: business_id = row['business_id_csv'] # if all review_id not in review_business_id_dict, then business_id in csv is used\n",
    "        for text, review_id, star in zip(texts, review_ids, stars):\n",
    "            ref_raw_dfs.append({'business_id': business_id, 'text': text, 'review_id': review_id, 'summary': summary, 'stars': star})\n",
    "        \n",
    "    ref_raw_df = pd.DataFrame(ref_raw_dfs)\n",
    "    ref_raw_df['tokens'] = ref_raw_df['text'].apply(get_tokens)\n",
    "    return ref_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_raw_df = get_ref_raw_df(config.ref_path, train_raw_df, val_tmp_df, test_tmp_df)\n",
    "assert len(ref_raw_df) == 200*config.n_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# group data by business id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group ref df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ref_group_df(data_df):\n",
    "    group_df = data_df.groupby('business_id').agg({\n",
    "        'text': lambda text_list: ' </DOC> '.join(list(text_list)),\n",
    "        'tokens': lambda token_idxs_list: [sent_idxs for token_idxs in list(token_idxs_list) for sent_idxs in token_idxs],\n",
    "        'summary': lambda summary_series: list(summary_series)[0], # only first column for each business id\n",
    "        'stars': lambda stars_list: list(stars_list)\n",
    "    })\n",
    "    group_df = group_df.reset_index()\n",
    "    group_df['doc_l'] = group_df['tokens'].apply(lambda tokens: len(tokens))\n",
    "    group_df['max_sent_l'] = group_df['tokens'].apply(lambda tokens: max([len(line) for line in tokens]))\n",
    "    group_df['sent_l'] = group_df['tokens'].apply(lambda tokens: [len(line) for line in tokens])\n",
    "    group_df['summary_tokens'] = group_df['summary'].apply(get_tokens)\n",
    "    group_df['summary_doc_l'] = group_df['summary_tokens'].apply(lambda tokens: len(tokens))\n",
    "    group_df['summary_max_sent_l'] = group_df['summary_tokens'].apply(lambda tokens: max([len(line) for line in tokens]))\n",
    "    \n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df = get_ref_group_df(ref_raw_df)\n",
    "assert len(ref_df) == 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## group train tmp df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_tmp_df(data_df, ref_df):\n",
    "    group_df = data_df.groupby('business_id').agg({\n",
    "        'tokens': lambda tokens_list: list(tokens_list),\n",
    "        'stars': lambda stars_list: list(stars_list)\n",
    "    })\n",
    "    group_df = group_df.reset_index()\n",
    "    ref_business_ids = ref_df['business_id'].values\n",
    "    n_pre = len(group_df)\n",
    "    group_df = group_df[group_df['business_id'].apply(lambda business_id: business_id not in ref_business_ids)]\n",
    "    n_post = len(group_df)\n",
    "    print('filtered %i business ids in references from train datasets' % (n_pre-n_post))\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered 166 business ids in references from train datasets\n"
     ]
    }
   ],
   "source": [
    "train_tmp_df = get_group_tmp_df(train_raw_df, ref_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## batch train df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_df(group_tmp_df, n_reviews, filter_sent_l=np.inf, filter_doc_l=np.inf, item_max_reviews=None, item_min_reviews=0,  min_std_stars=np.inf):\n",
    "    item_to_nreviews = {row.business_id: len(row.tokens) for _, row in group_tmp_df.iterrows()}\n",
    "    n_per_item = np.mean([n for n in item_to_nreviews.values() if n <= item_max_reviews])\n",
    "    n_per_item = math.ceil(n_per_item / n_reviews)\n",
    "    print('Each item will appear {} times'.format(n_per_item))\n",
    "\n",
    "    batch_list = []\n",
    "    for _, row in group_tmp_df.iterrows():\n",
    "        tokens_stars_list = [(sents, stars) for sents, stars in zip(row.tokens, row.stars) if max([len(sent) for sent in sents]) <= filter_sent_l] # filter\n",
    "        if len(tokens_stars_list) < item_min_reviews: continue\n",
    "        i_per_item = 0\n",
    "        while i_per_item < n_per_item:\n",
    "            batch_tokens_stars = random.sample(tokens_stars_list, n_reviews)\n",
    "            tokens = [sent for sents, _ in batch_tokens_stars for sent in sents]\n",
    "            doc_l = len(tokens)\n",
    "            sent_l = [len(sent) for sent in tokens]\n",
    "            max_sent_l = max(sent_l)\n",
    "            stars = [star for _, star in batch_tokens_stars]\n",
    "            mean_stars = np.mean(stars)\n",
    "            std_stars = np.std(stars)\n",
    "            if doc_l <= filter_doc_l:\n",
    "                batch_list.append({'business_id': row.business_id, 'tokens': tokens, 'doc_l': doc_l, 'sent_l': sent_l, 'max_sent_l': max_sent_l, \\\n",
    "                                                   'stars': stars, 'mean_stars': mean_stars, 'std_stars': std_stars})\n",
    "                i_per_item += 1\n",
    "    \n",
    "    assert len(batch_list) == len(item_to_nreviews) * n_per_item\n",
    "    \n",
    "    group_df = pd.DataFrame(batch_list)\n",
    "    return group_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each item will appear 12 times\n",
      "185496\n",
      "CPU times: user 34.6 s, sys: 1.8 s, total: 36.4 s\n",
      "Wall time: 35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Each item will appear 12 times\n",
    "# 185496\n",
    "# CPU times: user 34.6 s, sys: 1.8 s, total: 36.4 s\n",
    "# Wall time: 35 s\n",
    "train_df = get_group_df(train_tmp_df, n_reviews=config.n_reviews, filter_sent_l=config.filter_sent_l, filter_doc_l=config.filter_doc_l, \\\n",
    "                                            item_max_reviews=config.item_max_reviews, min_std_stars=config.min_std_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split ref df into val & test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = [111,  74, 106,  45, 143, 147,  56, 150, 184,  38,  61, 125, 116,\n",
    "                             58, 159, 182,  86,   2,  22, 126,  55,  20, 161, 118, 141,  57,\n",
    "                            123,  68, 164,  14, 122,  64,  53,  85, 135,  79, 163, 198, 109,\n",
    "                            110,  25, 115, 113, 114,  78,  94, 151,  88, 162, 176,  66, 136,\n",
    "                             62, 137, 158, 148, 171, 145,  52,   1,  82,   5, 173, 124, 190,\n",
    "                            129, 185,  67, 107,   3, 193, 132,  69,  31,  41,  11, 108, 167,\n",
    "                             96,  12, 139,  90,  23,  95,  21,   7,  54, 174,  65,  47, 194,\n",
    "                            181, 153, 199, 121, 165,  80,  44, 188,  36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = ref_df.iloc[test_indices]\n",
    "val_df = ref_df.drop(test_df.index)\n",
    "assert len(set(test_df.index) & set(val_df.index)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build token idxs for language modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.8 s, sys: 1.93 s, total: 23.7 s\n",
      "Wall time: 23.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words_list = train_df['tokens'].apply(lambda tokens: [token for line in tokens for token in line])\n",
    "word_tf_dict = sorted(Counter([word for words in words_list for word in words]).items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(word_tf_dict, max_tf_rate=0., min_tf_rate=0., min_tf=None, min_df=0., max_df=np.inf, stop_words=[]):\n",
    "    filtered_word_tf_dict = dict([(word, tf) for word, tf in word_tf_dict if word not in stop_words])\n",
    "    if min_tf: filtered_word_tf_dict = {word: tf for word, tf in filtered_word_tf_dict.items() if tf >= min_tf}\n",
    "    filtered_words = [word for word, _ in filtered_word_tf_dict.items()]        \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32956"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_words = [PAD, UNK, BOS, EOS]\n",
    "lm_words = special_words + filter_words(word_tf_dict, min_tf=config.min_tf) # large, usual\n",
    "idx_to_word = {idx: word for idx, word in enumerate(lm_words)}\n",
    "word_to_idx = {word: idx for idx, word in idx_to_word.items()}\n",
    "len(lm_words) # 32956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_token_idxs(tokens_series):\n",
    "    def get_token_idxs(tokens):\n",
    "        return [[word_to_idx[token] if token in word_to_idx else word_to_idx[UNK] for token in sent] for sent in tokens]\n",
    "    return tokens_series.apply(get_token_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.9 s, sys: 56.8 s, total: 1min 36s\n",
      "Wall time: 1min 38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isonuma/.pyenv/versions/anaconda3-5.3.1/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df['token_idxs'] = apply_parallel(train_df['tokens'], num_split=64, map_func=apply_token_idxs)\n",
    "val_df['token_idxs'] = apply_parallel(val_df['tokens'], num_split=64, map_func=apply_token_idxs)\n",
    "test_df['token_idxs'] = apply_token_idxs(test_df['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_save_df(data_df, summary=False):\n",
    "    save_df = data_df[['business_id', 'doc_l', 'sent_l', 'max_sent_l', 'token_idxs', 'tokens']]\n",
    "    if summary: save_df = data_df[['business_id', 'doc_l', 'sent_l', 'max_sent_l', 'tokens', 'token_idxs', \\\n",
    "                                                               'text', 'summary', 'summary_tokens', 'summary_doc_l', 'summary_max_sent_l']]\n",
    "    return save_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(185496, 100, 100)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (174794, 100, 100)\n",
    "train_save_df = get_save_df(train_df)\n",
    "val_save_df = get_save_df(val_df, summary=True)\n",
    "test_save_df = get_save_df(test_df, summary=True)\n",
    "len(train_save_df), len(val_save_df), len(test_save_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/yelp/yelp_recursum_df.pkl', 32956)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.output_path, len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving preprocessed instances...\n"
     ]
    }
   ],
   "source": [
    "print('saving preprocessed instances...')\n",
    "cPickle.dump((train_save_df, val_save_df, test_save_df, word_to_idx, idx_to_word, None),open(config.output_path,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
